{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from gsm import GSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# 1. データのロードとベクトル化\n",
    "newsgroups = fetch_20newsgroups(subset='all')\n",
    "texts = newsgroups.data\n",
    "labels = newsgroups.target\n",
    "\n",
    "# テキストデータのベクトル化\n",
    "vectorizer = CountVectorizer(max_features=1000, stop_words='english')\n",
    "X = vectorizer.fit_transform(texts).toarray()\n",
    "y = labels  # ラベルも用意\n",
    "\n",
    "# 2. カスタムデータセットクラスの作成\n",
    "class NewsgroupsDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = torch.tensor(data, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "# データセットの作成\n",
    "dataset = NewsgroupsDataset(X, y)\n",
    "\n",
    "# 3. DataLoaderの作成\n",
    "batch_size = 64\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GSM(1000, 128, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1, loss:669196606994.7722\n",
      "epoch:2, loss:24435.58054971695\n",
      "epoch:3, loss:8322.238621778786\n",
      "epoch:4, loss:7799.998302966356\n",
      "epoch:5, loss:7713.7074957937\n",
      "epoch:6, loss:7961.374050796032\n",
      "epoch:7, loss:7732.353099949658\n",
      "epoch:8, loss:7687.321512624621\n",
      "epoch:9, loss:7700.07757281512\n",
      "epoch:10, loss:7703.989817798138\n",
      "epoch:11, loss:7789.3257670253515\n",
      "epoch:12, loss:7640.9875332042575\n",
      "epoch:13, loss:7640.499278411269\n",
      "epoch:14, loss:7870.102637529373\n",
      "epoch:15, loss:7775.463030450046\n",
      "epoch:16, loss:7685.3477323800325\n",
      "epoch:17, loss:7799.037198789418\n",
      "epoch:18, loss:8005.076414488256\n",
      "epoch:19, loss:7592.69044046849\n",
      "epoch:20, loss:8174.856428913772\n",
      "epoch:21, loss:7674.508117079735\n",
      "epoch:22, loss:7745.1491726487875\n",
      "epoch:23, loss:7621.326284818351\n",
      "epoch:24, loss:7684.0814577788115\n",
      "epoch:25, loss:7593.285156816244\n",
      "epoch:26, loss:8105.596265561879\n",
      "epoch:27, loss:7625.577030979097\n",
      "epoch:28, loss:7637.367488741875\n",
      "epoch:29, loss:7777.698316968977\n",
      "epoch:30, loss:7633.863264761865\n",
      "epoch:31, loss:7626.040180698037\n",
      "epoch:32, loss:7542.653739973903\n",
      "epoch:33, loss:7625.5648051947355\n",
      "epoch:34, loss:8325.19836038351\n",
      "epoch:35, loss:7617.840220116079\n",
      "epoch:36, loss:7976.8750310614705\n",
      "epoch:37, loss:7635.017340034246\n",
      "epoch:38, loss:7746.0853339880705\n",
      "epoch:39, loss:7548.8030705451965\n",
      "epoch:40, loss:7728.608211576939\n",
      "epoch:41, loss:7598.3498664572835\n",
      "epoch:42, loss:7685.823694065213\n",
      "epoch:43, loss:7537.891261354089\n",
      "epoch:44, loss:7631.610086746514\n",
      "epoch:45, loss:7594.8622138127685\n",
      "epoch:46, loss:7580.517427667975\n",
      "epoch:47, loss:7647.387133337557\n",
      "epoch:48, loss:7699.832137256861\n",
      "epoch:49, loss:7540.299925662577\n",
      "epoch:50, loss:7563.044494763017\n",
      "epoch:51, loss:7540.722775943577\n",
      "epoch:52, loss:7639.817596450448\n",
      "epoch:53, loss:7620.808974295855\n",
      "epoch:54, loss:7540.605646274984\n",
      "epoch:55, loss:7627.68282494694\n",
      "epoch:56, loss:7534.852020420134\n",
      "epoch:57, loss:7544.539494358003\n",
      "epoch:58, loss:7685.943899661303\n",
      "epoch:59, loss:7588.145252682269\n",
      "epoch:60, loss:7532.120051302016\n",
      "epoch:61, loss:7525.27454598248\n",
      "epoch:62, loss:7523.8449463918805\n",
      "epoch:63, loss:7538.02383775264\n",
      "epoch:64, loss:7571.7913719415665\n",
      "epoch:65, loss:7550.399681933224\n",
      "epoch:66, loss:7518.158139780164\n",
      "epoch:67, loss:7513.8066430315375\n",
      "epoch:68, loss:7520.137712992728\n",
      "epoch:69, loss:7518.364729903638\n",
      "epoch:70, loss:7557.662420891225\n",
      "epoch:71, loss:7517.027592636645\n",
      "epoch:72, loss:7565.230595432222\n",
      "epoch:73, loss:7518.291669093072\n",
      "epoch:74, loss:7554.48963727802\n",
      "epoch:75, loss:7511.650876440108\n",
      "epoch:76, loss:7512.071411527693\n",
      "epoch:77, loss:7527.045180924237\n",
      "epoch:78, loss:7511.863440155983\n",
      "epoch:79, loss:7524.33288539201\n",
      "epoch:80, loss:7512.367740444839\n",
      "epoch:81, loss:7510.7696728259325\n",
      "epoch:82, loss:7513.891806825995\n",
      "epoch:83, loss:7528.246125809848\n",
      "epoch:84, loss:7521.660968437791\n",
      "epoch:85, loss:7507.25665756315\n",
      "epoch:86, loss:7502.691814258695\n",
      "epoch:87, loss:7502.528732076287\n",
      "epoch:88, loss:7502.391911886632\n",
      "epoch:89, loss:7502.35215292871\n",
      "epoch:90, loss:7502.347618795931\n",
      "epoch:91, loss:7502.357362963259\n",
      "epoch:92, loss:7502.334508366883\n",
      "epoch:93, loss:7502.300697296858\n",
      "epoch:94, loss:7502.603087224066\n",
      "epoch:95, loss:7502.255019448698\n",
      "epoch:96, loss:7502.253502674401\n",
      "epoch:97, loss:7502.244658574462\n",
      "epoch:98, loss:7502.228308618069\n",
      "epoch:99, loss:7502.233442589641\n",
      "epoch:100, loss:7502.220966361463\n"
     ]
    }
   ],
   "source": [
    "model.learn(dataloader, epoch=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, ( batch_data, batch_lbl ) in enumerate(dataloader):\n",
    "    x = batch_data.to(device=\"cuda\")\n",
    "    z, _ = model.encode(x)\n",
    "    topic = F.softmax(z, dim=1)\n",
    "    if i == 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5, 14,  5,  5,  5,  5,  5,\n",
       "         5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,\n",
       "         5,  5,  5,  5,  5,  5,  5,  5,  5,  5, 14,  5,  5,  5,  5,  5,  5,  5,\n",
       "        14,  5,  7, 14,  5,  5,  5,  5,  5,  5], device='cuda:0')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(topic, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312-cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
